# Methodology and Approach
This document outlines the technical methodology employed in the persona-based PDF content analysis pipeline. The approach is designed as a modular, multi-stage process that combines machine learning-based structural analysis with deep semantic understanding to extract and rank information with high relevance to a user's specified task. The entire process is orchestrated by main_workflow.py, which executes each component script in sequence.

### Phase 1: Structural and Content Extraction (p.py, c1.py, c2.py)
The initial phase focuses on deconstructing the PDF documents to understand their hierarchical layout and extract all textual content. This is a critical foundation for the subsequent semantic analysis.

**ML-Based Outline Generation (p.py):** The core of the structural analysis is performed by p.py. This script iterates through each PDF and uses the PyMuPDF library to extract text line-by-line, along with a rich set of features for each line. These features include:

Visual Cues: Font size, bold status, text case (uppercase, title case, etc.).

**Spatial Positioning:** Normalized X/Y coordinates, indentation, and vertical spacing between lines.

**Linguistic Features:** Part-of-speech counts (verbs, nouns, adjectives) generated using a locally-hosted spaCy model (en_core_web_sm).

This feature set is then fed into a two-step classification process using pre-trained LightGBM models:

* First, is_heading_classifier_model.pkl performs a binary classification to determine if a line is a heading or body text.

* Second, lines identified as headings are passed to heading_level_classifier_model.pkl, a multi-class classifier that assigns a hierarchical level (Title, H1, H2, H3). The output is a structured JSON outline for each PDF, which is saved to the r1_op directory.

**Data Aggregation (c1.py):** This script acts as a bridge, reading the main challenge1b_input.json and dynamically loading the corresponding structural outlines generated by p.py. It merges this information into a single, cohesive data structure for the next step.

**Full Text Ingestion (c2.py):** To ensure no content is missed, c2.py re-opens each PDF with PyMuPDF and extracts the complete, raw text from every page. This unstructured text is mapped by page number and integrated with the structural outlines from the previous step, creating a comprehensive data object that contains both hierarchical section data and the full text needed for context.

### Phase 2: Semantic Analysis via Vector Embeddings (final_c3.py)
With the documents fully parsed, the pipeline transitions from structural analysis to semantic understanding.

The core of this phase is the use of the sentence-transformers/all-MiniLM-L6-v2 model, loaded from a local models directory. This powerful model excels at converting text into high-dimensional vector embeddings that capture deep semantic meaning. The script generates embeddings for two key elements:

**The User Query:** The "job_to_be_done" text from challenge1b_input.json is converted into a target vector that represents the core informational need.

**Document Sections:** The pipeline intelligently defines a "section" as a heading's text concatenated with all the body text that follows it, up until the next heading of an equal or higher level. This contextual block of text is then converted into a corresponding section vector.

### Phase 3: Relevance Ranking and Output Synthesis (final_c3.py)
The final phase quantifies the relevance of each document section to the user's query and assembles the final output JSON.

**Cosine Similarity:** The script calculates the cosine similarity between the user's query vector and every section vector. This mathematical operation produces a relevance score for each section, indicating how closely its content aligns with the user's task.

**Ranking and Filtering:** All sections are ranked in descending order based on their similarity scores. A sophisticated filtering logic is then applied to prevent redundancy. This logic identifies parent-child relationships in the document outline and can prune less relevant child sections if a parent section already has a high similarity score, ensuring the output is both relevant and concise.

**Output Generation:** The top-ranked sections are compiled into the final challenge1b_output.json. This file includes the original metadata, a ranked list of the most important extracted_sections (with document name, section title, and page number), and a subsection_analysis containing the full, cleaned text of those top-ranked sections.